{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "XtS_1U7uWoSz"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -qqq tiktoken\n",
        "!pip install -qqq pinecone-client\n",
        "!pip install -qqq pypdf\n",
        "!pip install -qqq openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "CwMoVpfTX4e_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY=userdata.get('openai-key')\n",
        "PINECONE_API_KEY=userdata.get('pinecone-key')\n",
        "!pip install -qqq langchain_community\n",
        "from langchain_community.document_loaders.pdf import PyPDFDirectoryLoader\n"
      ],
      "metadata": {
        "id": "O3IVu_sJXhxL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd0354c-f243-411d-d663-e3015de70356"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
      ],
      "metadata": {
        "id": "H5P1KMv2noyB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "\n",
        "def read_doc(directory: str) -> list[str]:\n",
        "    # Function to read the PDFs from a directory.\n",
        "\n",
        "    # Args:\n",
        "    # directory (str): The path of the directory where the PDFs are stored.\n",
        "\n",
        "    # Returns:\n",
        "    # list[str]: A list of text in the PDFs.\n",
        "\n",
        "    page_contents = []\n",
        "\n",
        "    # List all PDF files in the directory\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "\n",
        "            # Open the PDF file\n",
        "            reader = PdfReader(filepath)\n",
        "            num_pages = len(reader.pages)\n",
        "\n",
        "            # Extract text from each page\n",
        "            for page_num in range(num_pages):\n",
        "                page = reader.pages[page_num]\n",
        "                page_contents.append(page.extract_text())\n",
        "\n",
        "    return page_contents\n",
        "\n",
        "# Call the function\n",
        "full_document = read_doc(\"/content\")\n",
        "print(len(full_document))  # This should print the number of pages read\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnZ7Xg3nXjFB",
        "outputId": "960a5112-d670-4c33-94a6-1de79b171972"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code for Creating the Embeddings for the whole file\n",
        "###But my ram is getting crashed so I used only 15 pages in my experiment"
      ],
      "metadata": {
        "id": "EvbThaowUusI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def generate_embeddings(documents: list[any]) -> list[list[float]]:\n",
        "\n",
        "    #Generate embeddings for a list of documents.\n",
        "\n",
        "    #Args:\n",
        "        #documents (list[any]): A list of document objects, each containing a 'page_content' attribute.\n",
        "\n",
        "    #Returns:\n",
        "        #list[list[float]]: A list containig a list of embeddings corresponding to the documents.\n",
        "\n",
        "    embedded = [embed_model.embed_documents(doc) for doc in documents]\n",
        "    return embedded\n",
        "\n",
        "\n",
        "# Run the function\n",
        "chunked_document_embeddings = generate_embeddings(documents=full_document)\n",
        "\n",
        "# Let's see the dimension of our embedding model so we can set it up later in pinecone\n",
        "print(len(chunked_document_embeddings))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EXAeUMdmahu",
        "outputId": "6fff03d8-5478-4b06-91db-dde90dd65566",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.10)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.35.7)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.82)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Embeddings for only 15 pages\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R56kRk6J1grp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Initialize the embedding model\n",
        "embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "def generate_embeddings(documents: list[str]) -> list[list[float]]:\n",
        "    # Generate embeddings for a list of documents.\n",
        "    # Args:\n",
        "    # documents (list[str]): A list of document objects, each containing a 'page_content' attribute.\n",
        "    # Returns:\n",
        "    # list[list[float]]: A list containing a list of embeddings corresponding to the documents.\n",
        "\n",
        "    embedded = [embed_model.embed_documents([doc]) for doc in documents]\n",
        "    return embedded\n",
        "\n",
        "# Extract the first 15 pages from full_document\n",
        "selected_documents = full_document[:15]\n",
        "\n",
        "# Run the function\n",
        "chunked_document_embeddings = generate_embeddings(documents=selected_documents)\n",
        "\n",
        "# Let's see the dimension of our embedding model so we can set it up later in pinecone\n",
        "print(len(chunked_document_embeddings))\n",
        "\n",
        "# Optionally, print the shape of one of the embeddings\n",
        "print(len(chunked_document_embeddings[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYNUj1lr1e_p",
        "outputId": "760ce9d7-9590-4728-a244-16ce534c8485"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.1.11)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.10)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.35.7)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.82)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.4)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain_openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "15\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create unique IDs\n",
        "ids = [str(x) for x in range(0,len(chunked_document_embeddings))]\n",
        "\n",
        "def combine_vector_and_text(\n",
        "    documents: list[any], doc_embeddings: list[list[float]]) -> list[dict[str, any]]:\n",
        "    \"\"\"\n",
        "    Process a list of documents along with their embeddings.\n",
        "\n",
        "    Args:\n",
        "    - documents (List[Any]): A list of documents (strings or other types).\n",
        "    - doc_embeddings (List[List[float]]): A list of embeddings corresponding to the documents.\n",
        "\n",
        "    Returns:\n",
        "    - data_with_metadata (List[Dict[str, Any]]): A list of dictionaries, each containing an ID, embedding values, and metadata.\n",
        "    \"\"\"\n",
        "    data_with_metadata = []\n",
        "\n",
        "    for id,doc_text, embedding in zip(ids,documents, doc_embeddings):\n",
        "        # Convert doc_text to string if it's not already a string\n",
        "        if not isinstance(doc_text, str):\n",
        "            doc_text = str(doc_text)\n",
        "\n",
        "        # Generate a unique ID based on the text content\n",
        "        doc_id = id\n",
        "\n",
        "        # Create a data item dictionary\n",
        "        data_item = {\n",
        "            \"id\": doc_id,\n",
        "            \"values\": embedding[0],\n",
        "            \"metadata\": {\"text\": doc_text},  # Include the text as metadata\n",
        "        }\n",
        "\n",
        "        # Append the data item to the list\n",
        "        data_with_metadata.append(data_item)\n",
        "\n",
        "    return data_with_metadata\n",
        "\n",
        "\n",
        "# Call the function\n",
        "all_meta_data = combine_vector_and_text(full_document, chunked_document_embeddings)"
      ],
      "metadata": {
        "id": "AoUAe4or9K5f"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone,ServerlessSpec\n",
        "pinecone = Pinecone()\n",
        "\n",
        "INDEX_NAME=\"rag-pdf\"\n",
        "if INDEX_NAME in [index.name for index in pinecone.list_indexes()]:\n",
        "  pinecone.delete_index(INDEX_NAME)\n",
        "\n",
        "pinecone.create_index(name=INDEX_NAME, dimension=1536, metric='cosine',\n",
        "  spec=ServerlessSpec(cloud='aws', region='us-east-1'))\n",
        "\n",
        "index = pinecone.Index(INDEX_NAME)"
      ],
      "metadata": {
        "id": "Pu2l9lz8_De6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5dXCyqCNXR",
        "outputId": "318a1872-aca7-4192-f944-b860b0112d14"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pinecone.data.index.Index object at 0x7c52fea2a440>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index.upsert(all_meta_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "Avxgo8LjCkWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc68d74c-9de5-4053-ac37-e0e85108e694"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 15}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Generate embeddings for the query\n",
        "def query_pinecone_index(query_embeddings: list, top_k=3, include_metadata: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Query a Pinecone index.\n",
        "\n",
        "    Args:\n",
        "    - query_embeddings (list): List of query embeddings.\n",
        "    - top_k (int): Number of nearest neighbors to retrieve (default: 3).\n",
        "    - include_metadata (bool): Whether to include metadata in the query response (default: True).\n",
        "\n",
        "    Returns:\n",
        "    - query_response (dict): Query response containing nearest neighbors.\n",
        "    \"\"\"\n",
        "    query_response = index.query(\n",
        "        vector=query_embeddings, top_k=top_k, include_metadata=include_metadata\n",
        "    )\n",
        "    return query_response"
      ],
      "metadata": {
        "id": "ZWZBJ6UMD3Eq"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_query_with_red_teaming(query: str, threshold: float = 0.3) -> str:\n",
        "    \"\"\"\n",
        "    Handle the query by determining if it is on-topic or off-topic using red teaming.\n",
        "\n",
        "    Args:\n",
        "    - query (str): User's query.\n",
        "    - threshold (float): Similarity threshold to classify questions as on-topic or off-topic.\n",
        "\n",
        "    Returns:\n",
        "    - str: Response to the query.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = embed_model.embed_query(query)\n",
        "\n",
        "    # Query the Pinecone index\n",
        "    query_response = query_pinecone_index(query_embeddings=query_embedding)\n",
        "\n",
        "    # Extract the similarities and metadata\n",
        "    similarities = [result['score'] for result in query_response['matches']]\n",
        "    documents = [result['metadata']['text'] for result in query_response['matches']]\n",
        "\n",
        "    # Check if the query is on-topic\n",
        "    max_similarity = max(similarities)\n",
        "    if max_similarity < threshold:\n",
        "        return \"The question is beyond the scope of the current documents.\"\n",
        "\n",
        "    # Find the most relevant document chunk\n",
        "    most_relevant_index = np.argmax(similarities)\n",
        "    return f\"{documents[most_relevant_index]}\"\n"
      ],
      "metadata": {
        "id": "tWyCN2rXpW8U"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "# Extract only the text from the dictionary before passing it to the LLM\n",
        "\n",
        "prompt = f\"{response} Using the provided information, give me a summarized answer\"\n",
        "\n",
        "def query_response(prompt: str) -> str:\n",
        "    \"\"\"This function returns a better response using LLM\n",
        "    Args:\n",
        "        prompt (str): The prompt template\n",
        "\n",
        "    Returns:\n",
        "        str: The actual response returned by the LLM\n",
        "    \"\"\"\n",
        "    answer = LLM.invoke(prompt)\n",
        "    return answer.content\n",
        "\n"
      ],
      "metadata": {
        "id": "X3r65UAWvbCU"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query1"
      ],
      "metadata": {
        "id": "f46iIm7sh4ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"What is the score of the match between India and Pakistan?\"\n",
        "response = handle_query_with_red_teaming(query)\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "  print(\"The question is beyond the scope of the current documents.\")\n",
        "else:\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from langchain.prompts import PromptTemplate\n",
        "\n",
        "  LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "  # Extract only the text from the dictionary before passing it to the LLM\n",
        "\n",
        "  prompt = f\"{response} Using the provided information, give me a summarized answer\"\n",
        "\n",
        "  def query_response(prompt: str) -> str:\n",
        "      \"\"\"This function returns a better response using LLM\n",
        "      Args:\n",
        "          prompt (str): The prompt template\n",
        "\n",
        "      Returns:\n",
        "          str: The actual response returned by the LLM\n",
        "      \"\"\"\n",
        "      answer = LLM.invoke(prompt)\n",
        "      return answer.content\n",
        "\n",
        "  # Printing response from red teaming technique\n",
        "  print(f\"\\n Response from the RAG: \\n {response}\")\n",
        "\n",
        "\n",
        "\n",
        "  # Call the LLM function\n",
        "  final_answer = query_response(prompt=prompt)\n",
        "  print(f\"\\n Response from the LLM: \\n {final_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-7nFchZENQ2",
        "outputId": "4663b922-9178-482b-cd13-bb143d02b0e3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Response from the RAG: \n",
            " The question is beyond the scope of the current documents.\n",
            "\n",
            " Response from the LLM: \n",
            " I'm sorry, but without specific information or context, I am unable to provide a summarized answer to a question that is beyond the scope of the current documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query2"
      ],
      "metadata": {
        "id": "2imEQy8LiC9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"What were Lufthansa's net profits and operating profits in 2004, and how do these figures compare to 2003?\"\n",
        "response = handle_query_with_red_teaming(query)\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "  print(\"The question is beyond the scope of the current documents.\")\n",
        "else:\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from langchain.prompts import PromptTemplate\n",
        "\n",
        "  LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "  # Extract only the text from the dictionary before passing it to the LLM\n",
        "\n",
        "  prompt = f\"{response} Using the provided information, give me a summarized answer\"\n",
        "\n",
        "  def query_response(prompt: str) -> str:\n",
        "      \"\"\"This function returns a better response using LLM\n",
        "      Args:\n",
        "          prompt (str): The prompt template\n",
        "\n",
        "      Returns:\n",
        "          str: The actual response returned by the LLM\n",
        "      \"\"\"\n",
        "      answer = LLM.invoke(prompt)\n",
        "      return answer.content\n",
        "\n",
        "  # Printing response from red teaming technique\n",
        "  print(f\"\\n Response from the RAG: \\n {response}\")\n",
        "\n",
        "\n",
        "\n",
        "  # Call the LLM function\n",
        "  final_answer = query_response(prompt=prompt)\n",
        "  print(f\"\\n Response from the LLM: \\n {final_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYLrfM05h-Lr",
        "outputId": "14361c43-3428-42d1-b0aa-294b3649ccb5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Response from the RAG: \n",
            " Lufthansa flies back to profit  \n",
            " \n",
            "German airline Lufthansa has returned to profit in 2004 after posting huge losses in 2003.  \n",
            "In a preliminary report, the airline announced net profits of 400m euros \n",
            "($527.61m; Â£274.73m), compared with a loss of 984m euros in 2003. \n",
            "Operating profits were at 380m euros, ten times more than in 2003. \n",
            "Lufthansa was hit in 2003 by tough competition and a dip in demand following the Iraq war and the killer SARS virus. It was also hit by troubles at its US catering business. Last year, Lufthansa showed signs of recovery even as some European and US airlines were teetering on the brink of bankruptcy. The board of Lufthansa has recommended paying a 2004 \n",
            "dividend of 0.30 euros per share. In 2003, shareholders did not get a \n",
            "dividend. The company said that it will give all the details of its 2004 \n",
            "results on 23 March.  \n",
            " \n",
            "-------------------------------------------------------------------------\n",
            "------- \n",
            "-------------------------------------------------------------------------------- \n",
            "Winn-Dixie files for bankruptcy  \n",
            " US supermarket group Winn- Dixie has filed for bankruptcy protection after \n",
            "succumbing to stiff competition in a market dominated by Wal -Mart. \n",
            " \n",
            "Winn-Dixie, once among the most profitable of US grocers, said Chapter 11 \n",
            "protection would enable it to successfully restructure. It said its 920 stores would remain open, but analysts said it would most likely off -load \n",
            "a number of sites. The Jacksonville, Florida- based firm has total debts \n",
            "of $1.87bn (Â£980m). In its bankruptcy petition it listed its biggest creditor as US foods giant Kraft Foods, which it owes $15.1m.  Analysts say Winn -Dixie had not kept up with consumers' demands and had \n",
            "also been burdened by a number of stores in need of upgrading. A 10- month \n",
            "restructuring plan was deemed a failure, and following a larger- than-\n",
            "expected quarterly loss earlier this month, Winn- Dixie's slide into \n",
            "bankruptcy was widely expected. The company's new chief executive Peter \n",
            "Lynch said Winn -Dixie would use the Chapter 11 breathing space to take \n",
            "the necessary action to turn itself around. \"This includes achieving \n",
            "significant cost reductions, improving the merchandising and customer service in all locations and generating a sense of excitement in the stores,\" he said. Yet Evan Mann, a senior bond analyst at Gimme Credit, said Mr Lynch's job would not be easy, as the bankruptcy would inevitably \n",
            "put off some customers. \"The real big issue is what's going to happen \n",
            "over the next one or two quarters now that they are in bankruptcy and all \n",
            "their customers see this in their local newspapers,\" he said.  \n",
            " \n",
            "-------------------------------------------------------------------------------- \n",
            "-------------------------------------------------------------------------------- \n",
            "US economy still growing says Fed  \n",
            " \n",
            "\n",
            " Response from the LLM: \n",
            " Lufthansa has returned to profit in 2004 after posting losses in 2003, with net profits of 400m euros. Winn-Dixie has filed for bankruptcy due to stiff competition and consumer demands, with debts of $1.87bn. The US economy is still growing, according to the Fed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query3"
      ],
      "metadata": {
        "id": "PPeMh-zSiFnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"What the result of yesterday t20 crciket worldcup seminal match?\"\n",
        "response = handle_query_with_red_teaming(query)\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "  print(\"The question is beyond the scope of the current documents.\")\n",
        "else:\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from langchain.prompts import PromptTemplate\n",
        "\n",
        "  LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "  # Extract only the text from the dictionary before passing it to the LLM\n",
        "\n",
        "  prompt = f\"{response} Using the provided information, give me a summarized answer\"\n",
        "\n",
        "  def query_response(prompt: str) -> str:\n",
        "      \"\"\"This function returns a better response using LLM\n",
        "      Args:\n",
        "          prompt (str): The prompt template\n",
        "\n",
        "      Returns:\n",
        "          str: The actual response returned by the LLM\n",
        "      \"\"\"\n",
        "      answer = LLM.invoke(prompt)\n",
        "      return answer.content\n",
        "\n",
        "  # Printing response from red teaming technique\n",
        "  print(f\"\\n Response from the RAG: \\n {response}\")\n",
        "\n",
        "\n",
        "\n",
        "  # Call the LLM function\n",
        "  final_answer = query_response(prompt=prompt)\n",
        "  print(f\"\\n Response from the LLM: \\n {final_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWszjFBxh-g8",
        "outputId": "fcd00f65-1a6b-4cd8-f621-1a2a648135dc"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The question is beyond the scope of the current documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Query4"
      ],
      "metadata": {
        "id": "7XG05U3qh_33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "query = \"How did Japan's industrial output and retail sales perform in January, and what impact did this have on economic recovery hopes and the stock market?\"\n",
        "response = handle_query_with_red_teaming(query)\n",
        "if response == \"The question is beyond the scope of the current documents.\":\n",
        "  print(\"The question is beyond the scope of the current documents.\")\n",
        "else:\n",
        "  from langchain_openai import ChatOpenAI\n",
        "  from langchain.prompts import PromptTemplate\n",
        "\n",
        "  LLM = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "  # Extract only the text from the dictionary before passing it to the LLM\n",
        "\n",
        "  prompt = f\"{response} Using the provided information, give me a summarized answer\"\n",
        "\n",
        "  def query_response(prompt: str) -> str:\n",
        "      \"\"\"This function returns a better response using LLM\n",
        "      Args:\n",
        "          prompt (str): The prompt template\n",
        "\n",
        "      Returns:\n",
        "          str: The actual response returned by the LLM\n",
        "      \"\"\"\n",
        "      answer = LLM.invoke(prompt)\n",
        "      return answer.content\n",
        "\n",
        "  # Printing response from red teaming technique\n",
        "  print(f\"\\n Response from the RAG: \\n {response}\")\n",
        "\n",
        "\n",
        "\n",
        "# Call the LLM function\n",
        "final_answer = query_response(prompt=prompt)\n",
        "print(f\"\\n Response from the LLM: \\n {final_answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZM2RGiZh-3Z",
        "outputId": "80f2f669-6ec1-4c3e-d0c3-b614c655848b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Response from the RAG: \n",
            " Analysts said that the move is aimed at winning over investors opposed to \n",
            "its bid for the London Stock Exchange. Critics of the takeover have complained that the money could be better used by returning cash to shareholders. Deutsche Boerse also said profit in the three months to 31 December was 120.7m euros ($158.8m; Â£83.3m). Sales climbed to 364.4m \n",
            "euros, lifting revenue for the year to a record 1.45bn euros.  \n",
            " \n",
            "Frankfurt -based Deutsche Boerse has offered Â£1.3bn ($2.48bn; 1.88bn \n",
            "euros) for the London Stock Exchange. Rival pan -European bourse Euronext \n",
            "is working also on a bid. Late on Monday, Deutsche Boerse said it would \n",
            "lift its 2004 dividend payment to 70 euro cents (Â£0.48; $0.98) from 55 euro cents a year earlier. \"There is a whiff of a sweetener in there,\" Anais Faraj, an analyst at Nomura told the BBC's World Business Report. \n",
            "\"Most of the disgruntled shareholders of Deutsche Boerse are complaining \n",
            "that the money that is being used for the bid could be better placed in \n",
            "their hands, paid out in dividends,\" Mr Faraj continued. Deutsche Boerse \n",
            "is \"trying to buy them off in a sense\", he said.  \n",
            " -------------------------------------------------------------------------------- \n",
            "-------------------------------------------------------------------------\n",
            "------- \n",
            "Japanese growth grinds to a halt  \n",
            " \n",
            "Growth in Japan evaporated in the three months to September, sparking renewed concern about an economy not long out of a decade- long trough. \n",
            " Output in the period grew just 0.1%, an annual rate of 0.3%. Exports -  \n",
            "the usual engine of recovery -  faltered, while domestic demand stayed \n",
            "subdued and corporate investment also fell short. The growth falls well short of expectations, but does mark a sixth straight quarter of expansion.  \n",
            " The economy had stagnated throughout the 1990s, experiencing only brief spurts of expansion amid long periods in the doldrums. One result was deflation - prices falling rather than rising - which made Japanese \n",
            "shoppers cautious and kept them from spending.  \n",
            " \n",
            "The effect was to leave the economy more dependent than ever on exports \n",
            "for its recent recovery. But high oil prices have knocked 0.2% off the growth rate, while the falling dollar means products shipped to the US are becoming relatively more expensive.  The performance for the third quarter marks a sharp downturn from earlier \n",
            "in the year. The first quarter showed annual growth of 6.3%, with the \n",
            "second showing 1.1%, and economists had been predicting as much as 2% \n",
            "this time around. \"Exports slowed while capital spending became weaker,\" \n",
            "said Hiromichi Shirakawa, chief economist at UBS Securities in Tokyo. \"Personal consumption looks good, but it was mainly due to temporary factors such as the Olympics. \"The amber light is flashing.\" The government may now find it more difficult to raise taxes, a policy it will have to implement when the economy picks up to help deal with Japan's massive public debt.  \n",
            "\n",
            " Response from the LLM: \n",
            " Deutsche Boerse is offering a higher dividend payment to appease disgruntled shareholders opposed to its bid for the London Stock Exchange. Meanwhile, Japan's economy saw growth slow in the third quarter, with exports faltering and domestic demand remaining subdued.\n"
          ]
        }
      ]
    }
  ]
}