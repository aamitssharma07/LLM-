{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Using any OpenAI model or Groq (llama 70b) model, solve the aisle mapping problem.  OpenAI code is provided here. You will do the usual 3 steps in terms of mounting the Google drive, your API key and install one of the LLM models, and import pandas.  Your goal is to do a model comparison and validation.\n",
        "**The code below works for OpenAI gpt-4o model.  Have not tested it on Llama model.  Also, this code with the dataset  may blow your budget if you are not careful with the size of test dataset, so exercise caution.**"
      ],
      "metadata": {
        "id": "f70xarX4M08z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2MgHRboqE8s",
        "outputId": "c534bea2-51d2-4974-f469-3c27580ffa18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qqq -r \"/content/drive/My Drive/LLMProjects/requirements.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL2tUtNlqIsr",
        "outputId": "19ee970b-89d2-4e39-b922-d07231c7ee23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "import os\n",
        "\n",
        "# Read and set the environment variable from the .bashrc file\n",
        "with open('/content/drive/My Drive/LLMProjects/.bashrc') as file:\n",
        "       for line in file:\n",
        "                if line.startswith('export '):\n",
        "                      var, value = line[len('export '):].strip().split('=')\n",
        "                      os.environ[var] = value"
      ],
      "metadata": {
        "id": "25BPtDT3qVj1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client2 = Groq()\n",
        "\n",
        "def get_completion(prompt, model=\"llama3-8b-8192\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = client2.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "dVInrUSNra0J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "# Path to your .bashrc file\n",
        "bashrc_path = '/content/drive/MyDrive/LLMProjects/.bashrc'\n",
        "\n",
        "# Read the .bashrc file and extract the API key\n",
        "with open(bashrc_path, 'r') as file:\n",
        "    lines = file.readlines()\n",
        "    for line in lines:\n",
        "        if line.startswith('export OPENAI_API_KEY='):\n",
        "            openai_api_key = line.split('=')[1].strip().strip('\"')\n",
        "\n",
        "# Set the OpenAI API key\n",
        "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "\n",
        "# Retrieve the OpenAI API key from the environment variables\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if openai_api_key is None:\n",
        "    raise ValueError(\"OpenAI API key not found. Please set it in the environment variables.\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Now you can use the OpenAI API with the set API key\n",
        "print(f\"OpenAI API key has been set successfully: {openai_api_key}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfwgLMXrrHd5",
        "outputId": "1970a7ec-caa5-4946-fd13-22357d0f03f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key has been set successfully: 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\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_NjmICpqpR2",
        "outputId": "2c47ae68-6943-4d6f-d63e-81658024f74d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QOYLii309HMZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel('Aisle-Mapping.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qaOJcT-l9NWw"
      },
      "outputs": [],
      "source": [
        "groceries = df['Grocery ITEM'].dropna().tolist()\n",
        "aisles = df['Aisle Category'].dropna().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-dQsDVjv-Ejn"
      },
      "outputs": [],
      "source": [
        "batch_size = 50  # Adjust the batch size as needed\n",
        "separator=\",\"\n",
        "client = openai.OpenAI()\n",
        "\n",
        "# Function to get the best matches for a batch of keywords using chat completion\n",
        "def get_best_matches_batch(grocery_batch, aisle_list):\n",
        "    prompt = \"Match each grocery item in the  grocery list with the most appropriate aisle category from the provided list of aisle categories.\\n\\n\"\n",
        "    prompt += \"The grocery list items are separated by commas. The list of aisles are also separated by commas. \\n\\n\"\n",
        "    prompt += \"List of grocery items to match:\\n\" + \"\\n\"+ \"\\n\"+separator.join(grocery_batch)+  \"\\n\\n\"\n",
        "    prompt += \"List of provided aisle categories:\\n\" + \"\\n\"+ \"\\n\"+separator.join(aisle_list)+ \"\\n\\n\"\n",
        "    prompt += \"If an appropriate aisle category is not to be found in the list, use Other \\n\\n\"\n",
        "    prompt += \"Return the matches in the format 'grocery item -> aisle category \\n\\n\"\n",
        "    prompt+=\"You must absolutely make sure that each grocery item is mapped to an aisle category\"\n",
        "    #print(prompt)\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that matches grocery items to aisle categories based on a typical grocery store or a supermarket in the USA.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=4096,\n",
        "        temperature=0.0\n",
        "    )\n",
        "    matches = response.choices[0].message.content\n",
        "    return matches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the dataset in batches\n",
        "results = []\n",
        "num_batches = len(groceries) // batch_size + (1 if len(groceries) % batch_size != 0 else 0)\n",
        "\n",
        "for i in range(num_batches):\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = min(start_idx + batch_size, len(groceries))\n",
        "    grocery_batch = groceries[start_idx:end_idx]\n",
        "\n",
        "    matches = get_best_matches_batch(grocery_batch, aisles)\n",
        "    results.append(matches)\n",
        "\n",
        "    # Print the batch number\n",
        "    print(f\"Batch {i+1} processed\")\n",
        "\n",
        "# Combine all results\n",
        "combined_results = \"\\n\".join(results)\n",
        "\n",
        "# Convert to JSON and store the results in a variable\n",
        "output_json = {\"matches\": combined_results}\n",
        "\n",
        "# Print only the top 50 results\n",
        "top_50_results = combined_results.split(\"\\n\")[:50]\n",
        "for idx, result in enumerate(top_50_results):\n",
        "    print(f\"Result {idx+1}: {result}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h05nNjPxtscZ",
        "outputId": "584c081f-4a1b-45e6-f5c9-85128b807c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 processed\n",
            "Batch 2 processed\n",
            "Batch 3 processed\n",
            "Batch 4 processed\n",
            "Batch 5 processed\n",
            "Batch 6 processed\n",
            "Batch 7 processed\n",
            "Batch 8 processed\n",
            "Batch 9 processed\n",
            "Batch 10 processed\n",
            "Batch 11 processed\n",
            "Batch 12 processed\n",
            "Batch 13 processed\n",
            "Batch 14 processed\n",
            "Batch 15 processed\n",
            "Batch 16 processed\n",
            "Batch 17 processed\n",
            "Batch 18 processed\n",
            "Batch 19 processed\n",
            "Batch 20 processed\n",
            "Batch 21 processed\n",
            "Batch 22 processed\n",
            "Batch 23 processed\n",
            "Batch 24 processed\n",
            "Batch 25 processed\n",
            "Batch 26 processed\n",
            "Batch 27 processed\n",
            "Batch 28 processed\n",
            "Batch 29 processed\n",
            "Batch 30 processed\n",
            "Batch 31 processed\n",
            "Batch 32 processed\n",
            "Batch 33 processed\n",
            "Batch 34 processed\n",
            "Batch 35 processed\n",
            "Batch 36 processed\n",
            "Batch 37 processed\n",
            "Batch 38 processed\n",
            "Batch 39 processed\n",
            "Batch 40 processed\n",
            "Batch 41 processed\n",
            "Batch 42 processed\n",
            "Batch 43 processed\n",
            "Batch 44 processed\n",
            "Batch 45 processed\n",
            "Batch 46 processed\n",
            "Batch 47 processed\n",
            "Batch 48 processed\n",
            "Batch 49 processed\n",
            "Batch 50 processed\n",
            "Batch 51 processed\n",
            "Batch 52 processed\n",
            "Batch 53 processed\n",
            "Batch 54 processed\n",
            "Batch 55 processed\n",
            "Batch 56 processed\n",
            "Batch 57 processed\n",
            "Batch 58 processed\n",
            "Batch 59 processed\n",
            "Batch 60 processed\n",
            "Batch 61 processed\n",
            "Batch 62 processed\n",
            "Batch 63 processed\n",
            "Batch 64 processed\n",
            "Batch 65 processed\n",
            "Batch 66 processed\n",
            "Batch 67 processed\n",
            "Batch 68 processed\n",
            "Batch 69 processed\n",
            "Batch 70 processed\n",
            "Batch 71 processed\n",
            "Batch 72 processed\n",
            "Batch 73 processed\n",
            "Batch 74 processed\n",
            "Batch 75 processed\n",
            "Batch 76 processed\n",
            "Batch 77 processed\n",
            "Batch 78 processed\n",
            "Batch 79 processed\n",
            "Batch 80 processed\n",
            "Batch 81 processed\n",
            "Batch 82 processed\n",
            "Batch 83 processed\n",
            "Batch 84 processed\n",
            "Batch 85 processed\n",
            "Batch 86 processed\n",
            "Batch 87 processed\n",
            "Batch 88 processed\n",
            "Batch 89 processed\n",
            "Batch 90 processed\n",
            "Batch 91 processed\n",
            "Batch 92 processed\n",
            "Batch 93 processed\n",
            "Batch 94 processed\n",
            "Batch 95 processed\n",
            "Batch 96 processed\n",
            "Batch 97 processed\n",
            "Batch 98 processed\n",
            "Batch 99 processed\n",
            "Batch 100 processed\n",
            "Batch 101 processed\n",
            "Batch 102 processed\n",
            "Batch 103 processed\n",
            "Batch 104 processed\n",
            "Batch 105 processed\n",
            "Batch 106 processed\n",
            "Batch 107 processed\n",
            "Batch 108 processed\n",
            "Batch 109 processed\n",
            "Batch 110 processed\n",
            "Batch 111 processed\n",
            "Batch 112 processed\n",
            "Batch 113 processed\n",
            "Batch 114 processed\n",
            "Batch 115 processed\n",
            "Batch 116 processed\n",
            "Batch 117 processed\n",
            "Batch 118 processed\n",
            "Batch 119 processed\n",
            "Batch 120 processed\n",
            "Batch 121 processed\n",
            "Batch 122 processed\n",
            "Batch 123 processed\n",
            "Batch 124 processed\n",
            "Batch 125 processed\n",
            "Batch 126 processed\n",
            "Batch 127 processed\n",
            "Batch 128 processed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract the Output from the First Model\n"
      ],
      "metadata": {
        "id": "_EfS9EZewkFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Example output from model 1 stored in a variable\n",
        "output_json = {\n",
        "    \"matches\": \"apple -> fruits\\nbanana -> fruits\\ncarrot -> vegetables\\nmilk -> dairy\\nbread -> bakery\\nchicken -> meat\\n\"\n",
        "}\n",
        "\n",
        "# Parse the output into a dictionary\n",
        "matches = output_json['matches'].split(\"\\n\")\n",
        "key_value_pairs = [match.split(\" -> \") for match in matches if match]\n",
        "grocery_aisle_dict = {kv[0]: kv[1] for kv in key_value_pairs}\n"
      ],
      "metadata": {
        "id": "eCEBZo32t7g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Suitable Prompt for Validation Using Groc"
      ],
      "metadata": {
        "id": "1O5H6BQ_ws4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate each (key, value) pair\n",
        "def validate_pair(grocery_item, aisle_category):\n",
        "    prompt = (\n",
        "        f\"Is it correct that the grocery item '{grocery_item}' belongs to the aisle '{aisle_category}'? \"\n",
        "        \"Answer 'yes' or 'no' and provide a brief explanation if needed.\"\n",
        "    )\n",
        "\n",
        "    response = get_completion(prompt)\n",
        "\n",
        "    validation_result = response.choices[0].message['content']\n",
        "    return validation_result\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7zTs4rlGv89F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validate Each (Key, Value) Pair"
      ],
      "metadata": {
        "id": "xFLG0LqiyShO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate each (key, value) pair\n",
        "validation_results = {}\n",
        "\n",
        "for grocery_item, aisle_category in grocery_aisle_dict.items():\n",
        "    validation_result = validate_pair(grocery_item, aisle_category)\n",
        "    validation_results[grocery_item] = validation_result\n",
        "\n",
        "# Print validation results\n",
        "for grocery_item, result in validation_results.items():\n",
        "    print(f\"{grocery_item} -> {result}\")\n"
      ],
      "metadata": {
        "id": "L5YSXXrsyWzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Observations Based on the Validation Results"
      ],
      "metadata": {
        "id": "moHhmDN5ybD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the validation results\n",
        "correct_validations = sum(1 for result in validation_results.values() if \"yes\" in result.lower())\n",
        "incorrect_validations = len(validation_results) - correct_validations\n",
        "\n",
        "print(f\"Correct Validations: {correct_validations}\")\n",
        "print(f\"Incorrect Validations: {incorrect_validations}\")\n",
        "# Calculate percentages\n",
        "total_validations = len(validation_results)\n",
        "correct_percentage = (correct_validations / total_validations) * 100\n",
        "incorrect_percentage = (incorrect_validations / total_validations) * 100\n",
        "\n",
        "# Print percentages\n",
        "print(f\"Correct Validations: {correct_percentage:.2f}%\")\n",
        "print(f\"Incorrect Validations: {incorrect_percentage:.2f}%\")\n"
      ],
      "metadata": {
        "id": "X4PF85ivyg3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "346n3TUPzK5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}